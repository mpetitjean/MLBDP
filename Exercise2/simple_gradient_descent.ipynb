{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From (404, 13) to (404, 13)\n",
      "From (102, 13) to (102, 13)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Step 1: Load dataset, split into training and test sets, and scale features\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "# load boston housing price dataset\n",
    "boston = load_boston()\n",
    "x = boston.data\n",
    "y = boston.target\n",
    "\n",
    "# split into training and test sets, namely 80 percent of examples goes for the training, 20 percent goes for the test set\n",
    "N_train = int(0.8 * x.shape[0])\n",
    "x_train = x[:N_train,:]\n",
    "y_train = y[:N_train]\n",
    "x_test = x[N_train:,:]\n",
    "y_test = y[N_train:]\n",
    "\n",
    "# scale features by removing mean and dividing by the standard deviation\n",
    "x_train_scaled = scale(x_train)\n",
    "x_test_scaled = scale(x_test)\n",
    "\n",
    "print(\"From\", x_train.shape, \"to\", x_train_scaled.shape)\n",
    "print(\"From\", x_test.shape, \"to\", x_test_scaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(404, 14)\n",
      "(102, 14)\n",
      "[[ 0.20889996]\n",
      " [ 0.35766823]\n",
      " [-0.04071453]\n",
      " [ 0.43840988]\n",
      " [-0.07440844]\n",
      " [ 0.7237008 ]\n",
      " [ 0.21786415]\n",
      " [-0.4361784 ]\n",
      " [-0.34416566]\n",
      " [-0.93819189]\n",
      " [-0.54629623]\n",
      " [ 0.0886206 ]\n",
      " [-0.59474134]\n",
      " [-1.33715585]]\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Add intercept terms and initialize parameters\n",
    "# Note: If you run this step again, please run from step 1 because notebook keeps the value from the previous run\n",
    "\n",
    "x_train_scaled = np.hstack((np.ones((x_train_scaled.shape[0], 1)), x_train_scaled))\n",
    "x_test_scaled = np.hstack((np.ones((x_test_scaled.shape[0], 1)), x_test_scaled))\n",
    "\n",
    "print(x_train_scaled.shape)\n",
    "print(x_test_scaled.shape)\n",
    "\n",
    "# init parameters using random values\n",
    "theta = np.random.normal(loc = 0, scale = 0.5, size = (1,x_train_scaled.shape[1]))\n",
    "theta = np.transpose(theta)\n",
    "print(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Implement the gradient and the cost function\n",
    "# In this step, you have to calculate the gradient. You can use the provided formula but the best way is to vectorize\n",
    "# that formula for efficiency\n",
    "def compute_gradient(x,y,theta):\n",
    "    y = np.reshape(y, (x.shape[0],1))\n",
    "    return 1/(x.shape[0]) * np.matmul( np.transpose(x), np.matmul(x,theta)-y )\n",
    "\n",
    "def compute_cost(x,y,theta):\n",
    "    y = np.reshape(y, (x.shape[0],1))\n",
    "    return 1/(2*x.shape[0]) * np.matmul( np.transpose(np.matmul(x,theta) - y), np.matmul(x,theta) - y )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of gradient squared error:  19032.7243352\n",
      "(14, 1)\n",
      "(14,)\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Verify the gradient value\n",
    "# In this step, you need to verify that the computed gradient is correct. The difference betweet the gradient and the\n",
    "# approximate gradient should be very small (~10^-18)\n",
    "\n",
    "\n",
    "def approximate_gradient(x,y,theta,epsilon):\n",
    "    n_features = x.shape[1]\n",
    "    app_grad = np.zeros(n_features)\n",
    "    epsilon_matrix = np.hstack(([epsilon], np.zeros(x.shape[1]-1)))\n",
    "    epsilon_matrix = np.reshape(epsilon_matrix, (x_test_scaled.shape[1],1))\n",
    "    for i in range(n_features):\n",
    "        a = compute_cost(x, y, theta + epsilon_matrix)\n",
    "        b = compute_cost(x, y, theta - epsilon_matrix)\n",
    "        app_grad[i] = (compute_cost(x, y, theta + epsilon_matrix) - compute_cost(x, y, theta - epsilon_matrix))/(2*epsilon)\n",
    "        epsilon_matrix = np.roll(epsilon_matrix, 1)\n",
    "    return app_grad\n",
    "\n",
    "\n",
    "#print(theta.shape)\n",
    "grad = compute_gradient(x_train_scaled,y_train,theta)\n",
    "epsilon = 1e-4\n",
    "app_grad = approximate_gradient(x_train_scaled,y_train,theta,epsilon)\n",
    "app_grad = np.reshape(app_, (x_test_scaled.shape[1],1))\n",
    "print('Sum of gradient squared error: ',np.sum((grad - app_grad)**2))\n",
    "\n",
    "print(grad.shape)\n",
    "print(app_grad.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Try gradient descent algorithm with different learning rates\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "# try different values for the learning rate\n",
    "learning_rates = [0.001,0.003,0.01,0.03,0.1,0.3]\n",
    "\n",
    "# this matrix keeps the learned parameters\n",
    "theta_matrix = np.zeros((len(learning_rates),x_train_scaled.shape[1]))\n",
    "\n",
    "# number of training iterations\n",
    "N_iterations = 100\n",
    "\n",
    "# prepare to plot\n",
    "plt.subplot(111)\n",
    "\n",
    "# calculate cost value and update theta\n",
    "for indx,alpha in enumerate(learning_rates):\n",
    "    # keep the cost value for each training step\n",
    "    J = np.zeros(N_iterations)\n",
    "    \n",
    "    # initialize new parameters using random distribution\n",
    "    theta = 0.5 * np.random.randn(x_train_scaled.shape[1],1)\n",
    "    for step in range(N_iterations):\n",
    "        # update theta\n",
    "        theta = theta - alpha * compute_gradient(x_train_scaled, y_train, theta)\n",
    "        \n",
    "        # save the value of theta\n",
    "        theta_matrix[indx,:] = np.reshape(theta, (x_train_scaled.shape[1]))\n",
    "        \n",
    "        # calculate the cost on traing set\n",
    "        J[step] = compute_cost(x_train_scaled, y_train, theta)\n",
    "    # plot cost function\n",
    "    plt.plot(J)\n",
    "plt.xlabel('Training step')\n",
    "plt.ylabel('Cost')\n",
    "plt.legend(('0.001','0.003','0.01','0.03','0.1','0.3'), loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Predict the price of house\n",
    "# You have to select the best theta you found\n",
    "theta = theta_matrix[-3,:]\n",
    "theta = np.reshape(theta, (x_test_scaled.shape[1],1))\n",
    "predict_price = np.matmul(x_test_scaled,theta)\n",
    "\n",
    "# calculate the cost for the test set\n",
    "test_cost = compute_cost(x_test_scaled, y_test, theta)\n",
    "print(test_cost.shape)\n",
    "print('test cost: ',test_cost)\n",
    "\n",
    "# plot the ground truth and the predicted\n",
    "x_axis = np.linspace(1,len(y_test),len(y_test))\n",
    "plt.plot(x_axis,y_test,'b',x_axis,predict_price,'r')\n",
    "plt.legend(('Ground truth','Predicted'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
